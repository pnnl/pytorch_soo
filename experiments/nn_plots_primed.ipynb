{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Iterable\n",
    "from pathlib import Path\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from tabulate import tabulate\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_DIR = \"./expt_rslts/primed/\"\n",
    "PICKLE_NAME = \"nn_rslts_primed_df.pkl\"\n",
    "IMG_OUTPUT_DIR = Path(\"imgs/\")\n",
    "IMG_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLE_OUTPUT_DIR = Path(\"tables/\")\n",
    "TABLE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OPT_OF_INTEREST = (\"sgd\", \"kn\", \"fr\", \"pr\", \"hs\", \"dy\", \"bfgs\")\n",
    "DISCRIMINATE_FGD = False\n",
    "if DISCRIMINATE_FGD:\n",
    "    OPT_OF_INTEREST = (\"fgd\", \"sgd\", \"kn\", \"fr\", \"pr\", \"hs\", \"dy\", \"bfgs\")\n",
    "\n",
    "# Truncate the opts of interest...\n",
    "OPT_OF_INTEREST = (\"kn\", \"fr\", \"bfgs\")\n",
    "\n",
    "\n",
    "DROP_N_EQUALS_TWO = True\n",
    "\n",
    "NAME_DICT = {\n",
    "    \"sgd\": \"Stochastic Gradient Descent\",\n",
    "    \"fgd\": \"Full Gradient Descent\",\n",
    "    \"kn\": \"Krylov-Newton\",\n",
    "    \"fr\": \"Fletcher-Reeves\",\n",
    "    \"pr\": \"Polak-Ribiere\",\n",
    "    \"hs\": \"Hestenes-Stiefel\",\n",
    "    \"dy\": \"Dai-Yuan\",\n",
    "    \"bfgs\": \"BFGS\",\n",
    "    \"bfgsi\": \"BFGS Inverse\",\n",
    "    \"dfp\": \"Davidon-Fletcher-Powell\",\n",
    "    \"dfpi\": \"Davidon-Fletcher-Powell Inverse\",\n",
    "    \"sr1\": \"Symmetric Rank-One\",\n",
    "    \"sr1d\": \"Symmetric Rank-One Dual\",\n",
    "    \"levenberg\": \"Levenberg\"\n",
    "}\n",
    "\n",
    "def read_json_to_df(fname):\n",
    "    try:\n",
    "        with open(fname) as f:\n",
    "            tmp_df = pd.json_normalize(json.load(f))\n",
    "            return tmp_df\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "if not os.path.exists(PICKLE_NAME):\n",
    "    print(\"Creating dataframe!\")\n",
    "    assert(os.path.exists(DATA_DIR))\n",
    "    f_list = glob.glob(os.path.join(DATA_DIR, \"TEST_priming_*.json\"))\n",
    "    print(\"Number of files:\", len(f_list))\n",
    "    with mp.Pool() as p:\n",
    "        dframes = p.map(read_json_to_df, f_list)\n",
    "    #dframes = [read_json_to_df(i) for i in f_list]\n",
    "    dframes = [i for i in dframes if i is not None]\n",
    "    monolith = pd.concat(dframes)\n",
    "    monolith.to_pickle(PICKLE_NAME)\n",
    "\n",
    "else:\n",
    "    print(\"Reading dataframe!\")\n",
    "    monolith = pd.read_pickle(PICKLE_NAME)\n",
    "\n",
    "monolith.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimizers:\", list(set(monolith[\"specs.opt\"])))\n",
    "print(\"Learning rates:\", list(set(monolith[\"specs.learning_rate\"])))\n",
    "print(\"Learning rates:\", list(set(monolith[\"specs.momentum\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monolith.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = monolith.loc[monolith[\"specs.dataset\"] == \"cifar10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar[\"specs.batch_size_train\"] = cifar[\"specs.batch_size_train\"].apply(int)\n",
    "if DISCRIMINATE_FGD:\n",
    "    cifar.loc[(cifar[\"specs.opt\"] == \"sgd\") & (cifar[\"specs.batch_size_train\"] >= 50000), \"specs.opt\"] = \"fgd\"\n",
    "    cifar.loc[cifar[\"specs.opt\"] == \"fgd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_acc_lambda(row):\n",
    "    try:\n",
    "        max_ = max(row[\"test_accuracy_list\"])\n",
    "    except ValueError:\n",
    "        max_ = 0.0\n",
    "    return max_\n",
    "\n",
    "def final_acc_lambda(row):\n",
    "    try:\n",
    "        last = row[\"test_accuracy_list\"][-1]\n",
    "    except IndexError:\n",
    "        last = float(\"NaN\")\n",
    "    return last\n",
    "\n",
    "cumulative_time_lambda = lambda row: np.cumsum(row[\"time\"])\n",
    "\n",
    "def total_time_lambda(row):\n",
    "    try:\n",
    "        max_ = row[\"training_timestamps\"][-1]\n",
    "    except IndexError:\n",
    "        max_ = float(\"NaN\")\n",
    "    return max_\n",
    "\n",
    "def time_to_peak_lambda(row):\n",
    "    try:\n",
    "        max_idx = np.array(row[\"test_accuracy_list\"]).argmax()\n",
    "        time_to_peak = np.sum(row[\"training_timestamps\"][:max_idx+1])\n",
    "\n",
    "    except ValueError:\n",
    "        time_to_peak = np.inf\n",
    "\n",
    "    return time_to_peak\n",
    "\n",
    "\n",
    "def fgd_lambda(row):\n",
    "    try:\n",
    "        opt_name = row[\"specs.opt\"]\n",
    "        batch_size_train = int(row[\"specs.batch_size_train\"])\n",
    "        if opt_name == \"sgd\" and batch_size_train >= 50000:\n",
    "            row[\"specs.opt\"] = \"fgd\"\n",
    "    except ValueError:\n",
    "        print(f\"row failed: {row['specs.opt']}\")\n",
    "\n",
    "\n",
    "def apply_lambda(df: pd.DataFrame, colname: str, func: Callable):\n",
    "    tmp = df.apply(func, axis=1)\n",
    "    df[colname] = tmp.copy()\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_summary_vals(df: pd.DataFrame):\n",
    "    df = apply_lambda(df, \"top_test_acc\", max_acc_lambda)\n",
    "    df = apply_lambda(df, \"final_test_acc\", final_acc_lambda)\n",
    "    df = apply_lambda(df, \"training_timestamps\", cumulative_time_lambda)\n",
    "    df = apply_lambda(df, \"total_training_time\", total_time_lambda)\n",
    "    df = apply_lambda(df, \"time_to_peak_acc\", time_to_peak_lambda)\n",
    "    df = df.dropna(subset=[\"total_training_time\"])\n",
    "    if DROP_N_EQUALS_TWO:\n",
    "        df = df[df[\"specs.batch_size_train\"] > 2]\n",
    "\n",
    "    return df\n",
    "\n",
    "cifar = get_summary_vals(cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_best_acc(full_df, n, total_time_filter=100000000):\n",
    "    full_df.sort_values([\"top_test_acc\"], ascending=False, inplace=True)\n",
    "    tmp = full_df.loc[full_df[\"total_training_time\"] < total_time_filter]\n",
    "    return tmp.head(n)\n",
    "\n",
    "def get_n_best_final_acc(full_df, n, total_time_filter=10000):\n",
    "    full_df.sort_values([\"final_test_acc\"], ascending=False, inplace=True)\n",
    "    tmp = full_df.loc[full_df[\"total_training_time\"] < total_time_filter]\n",
    "    return tmp.head(n)\n",
    "\n",
    "def get_n_random(full_df: pd.DataFrame, n: int):\n",
    "    subsample = full_df.sample(n=n, replace=True)\n",
    "    return subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rows(df: pd.DataFrame, best_column: str, opt_name: str, save=False, time=True, epoch=True, random=False):\n",
    "    save_args = dict(facecolor=\"white\", transparent=False)\n",
    "    top_or_rand_str = \"top\"\n",
    "    if random:\n",
    "        top_or_rand_str = \"random\"\n",
    "\n",
    "    plt.figure(figsize=(16,9))\n",
    "    if epoch:\n",
    "        for row in df.itertuples():\n",
    "            plt.plot(row.test_accuracy_list)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Percent correct\")\n",
    "        plt.ylim((0, 100.0))\n",
    "        title = f\"{opt_name}, {top_or_rand_str.capitalize()} {len(df.index)} runs by {best_column}, Accuracy vs. Epoch\"\n",
    "        plt.title(title)\n",
    "        if save:\n",
    "            save_name = f\"{opt_name}_{top_or_rand_str}_{len(df.index)}_{best_column}_acc_vs_epoch.png\"\n",
    "        plt.savefig(IMG_OUTPUT_DIR / save_name, **save_args)\n",
    "\n",
    "    if time:\n",
    "        plt.figure(figsize=(16,9))\n",
    "        for row in df.itertuples():\n",
    "            plt.plot(row.training_timestamps, row.test_accuracy_list)\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Percent correct\")\n",
    "        plt.ylim((0, 100.0))\n",
    "        title = f\"{opt_name}, {top_or_rand_str.capitalize()} {len(df.index)} runs by {best_column}, Accuracy vs. Time\"\n",
    "        plt.title(title)\n",
    "        if save:\n",
    "            save_name = f\"{opt_name}_{top_or_rand_str}_{len(df.index)}_{best_column}_acc_vs_time.png\"\n",
    "            plt.savefig(IMG_OUTPUT_DIR / save_name, **save_args)\n",
    "\n",
    "    if epoch:\n",
    "        plt.figure(figsize=(16,9))\n",
    "        for row in df.itertuples():\n",
    "            plt.plot(row.train_loss_list)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Training Loss, Log\")\n",
    "        plt.yscale(\"log\")\n",
    "        title = f\"{opt_name}, {top_or_rand_str.capitalize()} {len(df.index)} runs by {best_column}, Training Loss vs. Epoch\"\n",
    "        plt.title(title)\n",
    "        if save:\n",
    "            save_name = f\"{opt_name}_{top_or_rand_str}_{len(df.index)}_{best_column}_train_loss_vs_epoch.png\"\n",
    "            plt.savefig(IMG_OUTPUT_DIR / save_name, **save_args)\n",
    "\n",
    "        plt.figure(figsize=(16,9))\n",
    "        for row in df.itertuples():\n",
    "            plt.plot(row.test_loss_list)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Test Loss, Log\")\n",
    "        plt.yscale(\"log\")\n",
    "        title = f\"{opt_name}, {top_or_rand_str.capitalize()} {len(df.index)} runs by {best_column}, Test Loss vs. Epoch\"\n",
    "        plt.title(title)\n",
    "        if save:\n",
    "            save_name = f\"{opt_name}_{top_or_rand_str}_{len(df.index)}_{best_column}_test_loss_vs_epoch.png\"\n",
    "            plt.savefig(IMG_OUTPUT_DIR / save_name, **save_args)\n",
    "\n",
    "\n",
    "def plot_performance_vs_batch_size(df: pd.DataFrame, opt_name: str, peak: bool, violin: bool=False):\n",
    "    batch_sizes = [100, 1000, 5000, 10000, 25000, 50000]\n",
    "    opt_df = df.loc[df[\"specs.opt\"] == opt_name]\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "    if peak:\n",
    "        acc = opt_df[\"top_test_acc\"]\n",
    "        ylabel = \"Peak Test Accuracy\"\n",
    "    else:\n",
    "        acc = opt_df[\"final_test_acc\"]\n",
    "        ylabel = \"Final Test Accuracy\"\n",
    "\n",
    "    x_axis = opt_df[\"specs.batch_size_train\"]\n",
    "\n",
    "    if violin:\n",
    "        tmp_df = pd.concat([x_axis, acc], axis=1, keys=[\"hparam\", \"acc\"])\n",
    "        hparams = list(set(x_axis))\n",
    "        if len(hparams) == 0:\n",
    "            print(f\"No data for optimizer {opt_name}!\")\n",
    "            return\n",
    "        hparams.sort()\n",
    "        def get_hparam_match_and_count(hparam_):\n",
    "            tmp = tmp_df.loc[tmp_df[\"hparam\"] == hparam_][\"acc\"]\n",
    "            if len(tmp) == 0:\n",
    "                return pd.Series(np.zeros(2)*np.nan), 0\n",
    "            return tmp, len(tmp)\n",
    "        data_and_counts = [get_hparam_match_and_count(i) for i in batch_sizes]\n",
    "        data = [i[0] for i in data_and_counts]\n",
    "        counts = [i[1] for i in data_and_counts]\n",
    "        hparam_strs = [f\"{h}\\nN={c}\" for h, c in zip(batch_sizes, counts)]\n",
    "        xticklabels = hparam_strs\n",
    "        xticks = [*range(1,7)]\n",
    "\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.violinplot(data, showmedians=True)\n",
    "        ax.set_xticks(xticks)\n",
    "        ax.set_xticklabels(xticklabels)\n",
    "    else:\n",
    "        plt.scatter(x_axis, acc)\n",
    "\n",
    "    plt.xlabel(\"Batch Size\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.ylim((0.0, 100.0))\n",
    "    name = f\"{opt_name}_{ylabel}_vs_batch_size\".replace(\" \", \"_\")\n",
    "    name = name.replace(\"specs.\", \"\")\n",
    "    plot_title = name.split(\"_\")\n",
    "    plot_title[0] = plot_title[0].upper()\n",
    "\n",
    "    plt.title(\" \".join(plot_title).title())\n",
    "    plt.savefig(IMG_OUTPUT_DIR / (name + \".png\"), facecolor=\"white\", transparent=False)\n",
    "\n",
    "for optim in OPT_OF_INTEREST:\n",
    "    plot_performance_vs_batch_size(cifar, optim, True, violin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_optimizer_of_interest(df: pd.DataFrame, opt_name: str, to_plot: int, time=True, epoch=True, random=False) -> None:\n",
    "    opt_df = df.loc[df[\"specs.opt\"] == opt_name]\n",
    "    if random:\n",
    "        to_plot = get_n_random(opt_df, to_plot)\n",
    "    else:\n",
    "        to_plot = get_n_best_acc(opt_df, to_plot)\n",
    "    A = \"top_test_acc\"\n",
    "    B = \"final_test_acc\"\n",
    "    plot_rows(to_plot, A, opt_name, save=True, time=time, epoch=epoch, random=random)\n",
    "    #plot_rows(best_final_val, B, opt_name, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for opt in OPT_OF_INTEREST:\n",
    "    plot_optimizer_of_interest(cifar, opt, 100, epoch=True, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violin_opt_comparison(\n",
    "    full_df: pd.DataFrame,\n",
    "    opts_of_interest: Iterable[str],\n",
    "    save: bool=True,\n",
    ") -> None:\n",
    "    dataframes = []\n",
    "    for opt_name in opts_of_interest:\n",
    "        acc_series = full_df.loc[full_df[\"specs.opt\"] == opt_name][\"top_test_acc\"]\n",
    "        tmp_df = acc_series.to_frame(name=NAME_DICT[opt_name])\n",
    "        tmp_df.reset_index(inplace=True)\n",
    "        tmp_df.drop(\"index\", inplace=True, axis=1)\n",
    "        dataframes.append(tmp_df)\n",
    "\n",
    "    labels = [i.columns[0] for i in dataframes]\n",
    "    dataset = [i[i.columns[0]] for i in dataframes]\n",
    "    plt.figure(figsize=(15, 9))\n",
    "    plt.violinplot(dataset, showextrema=True, showmedians=True)\n",
    "    plt.xticks([*range(1, len(dataset)+1)], labels=labels)\n",
    "    plt.ylim((0.0, 100.0))\n",
    "    plt.ylabel(\"Optimizer\")\n",
    "    plt.ylabel(\"Peak Accuracy, %\")\n",
    "    plt.show()\n",
    "\n",
    "violin_opt_comparison(cifar, OPT_OF_INTEREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violin_plot_peak_acc(\n",
    "    df: pd.DataFrame,\n",
    "    opt_name: str,\n",
    "    discriminate_fgd: bool=False,\n",
    "    save=True\n",
    ") -> None:\n",
    "    nrows = 1\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=1, figsize=(16,9), squeeze=False)\n",
    "    axes[0, 0].set_ylim(bottom=0.0, top=100.0)\n",
    "    if discriminate_fgd and opt_name == \"sgd\":\n",
    "        opt_df = df.loc[df[\"specs.opt\"] == opt_name]\n",
    "        opt_df = opt_df.append(df.loc[df[\"specs.opt\"] == \"fgd\"], ignore_index=True)\n",
    "    else:\n",
    "        opt_df = df.loc[df[\"specs.opt\"] == opt_name]\n",
    "    #opt_df = get_summary_vals(opt_df)\n",
    "\n",
    "    # Peak accuracy\n",
    "    peak_acc = opt_df[\"top_test_acc\"]\n",
    "    axes[0, 0].violinplot(peak_acc, showextrema=True, showmedians=True)\n",
    "    axes[0, 0].set_title(f\"Distribution of Peak Testing Accuracy: {opt_name}\")\n",
    "\n",
    "    # Peak Accuracy Table\n",
    "    acc_summary = pd.DataFrame(peak_acc.describe())\n",
    "    acc_summary.rename(columns={\"top_test_acc\": \"Peak Test Accuracy\"}, inplace=True)\n",
    "\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(IMG_OUTPUT_DIR / f\"{opt}_peak_accuracy_violin.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for opt in OPT_OF_INTEREST:\n",
    "    violin_plot_peak_acc(cifar, opt, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_peak_vs_peak_acc(df: pd.DataFrame, opt_name: str, plot_full: bool=False, hist: bool=True):\n",
    "    opt_df = df.loc[df[\"specs.opt\"] == opt_name][[\"top_test_acc\", \"time_to_peak_acc\"]]\n",
    "    opt_df[opt_df[\"time_to_peak_acc\"] == np.inf] = np.nan\n",
    "\n",
    "    opt_df = opt_df.dropna()\n",
    "\n",
    "    peak = max(opt_df[\"top_test_acc\"])\n",
    "    peak_acc_row = opt_df.loc[opt_df[\"top_test_acc\"] == peak]\n",
    "    time_of_shortest_peak = min(peak_acc_row[\"time_to_peak_acc\"].values)\n",
    "    time_to_peak_acc = deepcopy(opt_df[\"time_to_peak_acc\"])\n",
    "    top_test_acc = deepcopy(opt_df[\"top_test_acc\"])\n",
    "\n",
    "    time_to_peak_acc[time_to_peak_acc == np.inf] = np.nan\n",
    "\n",
    "    if plot_full:\n",
    "        plt.figure(figsize=(16, 9))\n",
    "        plt.scatter(time_to_peak_acc, top_test_acc)\n",
    "        plt.ylim((0, 100))\n",
    "        plt.title(f\"{NAME_DICT[opt_name]}, Time to Peak Accuracy vs. Peak Accuracy\")\n",
    "        plt.axhline(y=peak, color='g', linestyle=\"-\", label=f\"Peak Accuracy: {peak}%\")\n",
    "        plt.axvline(x=time_of_shortest_peak, color=\"r\", linestyle=\"-\", label=f\"Shortest time to Peak Accuracy: {time_of_shortest_peak:.0f} seconds\")\n",
    "        plt.xlabel(\"Time to peak accuracy (seconds)\")\n",
    "        plt.ylabel(\"Peak Test Accuracy (%)\")\n",
    "        plt.legend()\n",
    "\n",
    "    if hist:\n",
    "        plt.figure(figsize=(16,9))\n",
    "        plt.hist2d(time_to_peak_acc, top_test_acc, bins=50)\n",
    "        plt.ylabel(\"Peak Test Accuracy, %\")\n",
    "        plt.xlabel(\"Time To Peak Accuracy, seconds\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.ylim((0, 100))\n",
    "    plt.scatter(time_to_peak_acc, top_test_acc)\n",
    "    plt.xlim((-30000, 650000))\n",
    "    plt.title(f\"{NAME_DICT[opt_name]}, Time to Peak Accuracy vs. Peak Accuracy, Windowed\")\n",
    "    plt.axhline(y=peak, color='g', linestyle=\"-\", label=f\"Peak Accuracy: {peak}%\")\n",
    "    plt.axvline(x=time_of_shortest_peak, color=\"r\", linestyle=\"-\", label=f\"Shortest time to Peak Accuracy: {time_of_shortest_peak:.0f} seconds\")\n",
    "    plt.xlabel(\"Time to peak accuracy (seconds)\")\n",
    "    plt.ylabel(\"Peak Test Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return time_of_shortest_peak, peak\n",
    "\n",
    "def plot_solution_space(solution_space):\n",
    "    opts = [i[0] for i in solution_space]\n",
    "    ts = [i[1] for i in solution_space]\n",
    "    ps = [i[2] for i in solution_space]\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.scatter(ts, ps)\n",
    "    for i, opt in enumerate(opts):\n",
    "        plt.annotate(opt, (ts[i], ps[i]))\n",
    "\n",
    "    plt.xlabel(\"Time to Best Peak Accuracy (seconds)\")\n",
    "    plt.ylabel(\"Best Peak Accuracy (%)\")\n",
    "\n",
    "solution_space = []\n",
    "for opt in OPT_OF_INTEREST:\n",
    "    t, p = time_to_peak_vs_peak_acc(cifar, opt)\n",
    "    solution_space.append((opt, t, p))\n",
    "\n",
    "plot_solution_space(solution_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway, kruskal, alexandergovern\n",
    "\n",
    "def conduct_test_intra_optimizer(df: pd.DataFrame, opt_name: str, test: Callable, batch_sizes: Optional[List[int]]=None):\n",
    "    opt_df = df.loc[df[\"specs.opt\"] == opt_name]\n",
    "    if batch_sizes is None:\n",
    "        batch_sizes = sorted(list(set(opt_df[\"specs.batch_size_train\"])))\n",
    "    \n",
    "    batch_size_rslts = [opt_df.loc[opt_df[\"specs.batch_size_train\"] == i][\"top_test_acc\"] for i in batch_sizes]\n",
    "    #batch_size_rslts = [opt_df.loc[df[\"specs.batch_size_train\"] == i] for i in batch_sizes]\n",
    "    results = test(*batch_size_rslts)\n",
    "\n",
    "    return results\n",
    "\n",
    "for opt in OPT_OF_INTEREST:\n",
    "    print(f\"{opt}:\", conduct_test_intra_optimizer(cifar, opt, f_oneway))\n",
    "\n",
    "for opt in OPT_OF_INTEREST:\n",
    "    print(f\"{opt}:\", conduct_test_intra_optimizer(cifar, opt, kruskal))\n",
    "\n",
    "for opt in OPT_OF_INTEREST:\n",
    "    print(f\"{opt}:\", conduct_test_intra_optimizer(cifar, opt, alexandergovern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ml20_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "0167b71c06170b09ecced200e50d8e6e90e424ceb5d30d6c8b1ca324c62a9c08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
